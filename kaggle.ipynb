{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hnpathak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import os \n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "TRAIN_SAMPLE_SIZE = 800000\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples\n",
      "        0   1  2   3    4                    5   6  7\n",
      "0   83230   3  1  13  379  2017-11-06 14:32:21 NaN  0\n",
      "1   17357   3  1  19  379  2017-11-06 14:33:34 NaN  0\n",
      "2   35810   3  1  13  379  2017-11-06 14:34:12 NaN  0\n",
      "3   45745  14  1  13  478  2017-11-06 14:34:52 NaN  0\n",
      "4  161007   3  1  13  379  2017-11-06 14:35:08 NaN  0\n",
      "(737196, 8)\n",
      "\n",
      " Positive samples\n",
      "        0   1  2   3    4                    5                    6  7\n",
      "0  204158  35  1  13   21  2017-11-06 15:41:07  2017-11-07 08:17:19  1\n",
      "1   29692   9  1  22  215  2017-11-06 16:00:02  2017-11-07 10:05:22  1\n",
      "2   64516  35  1  13   21  2017-11-06 16:00:02  2017-11-06 23:40:50  1\n",
      "3  172429  35  1  46  274  2017-11-06 16:00:03  2017-11-07 00:55:29  1\n",
      "4  199085  35  1  13  274  2017-11-06 16:00:04  2017-11-06 23:04:54  1\n",
      "(456846, 8)\n"
     ]
    }
   ],
   "source": [
    "# data is divided into positive negative samples and 7 gb file of negatives is further divided into 250 splits. \n",
    "# $ split -n l/250 negatives.csv\n",
    "print('Negative samples')\n",
    "data_neg = pd.read_csv('data/negatives/xaa', header=None)\n",
    "print(data_neg.head())\n",
    "print(data_neg.shape)\n",
    "print('\\n Positive samples')\n",
    "data_pos = pd.read_csv('data/positives/positives.csv',header=None)\n",
    "print(data_pos.head())\n",
    "print(data_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_combined:  (1194042, 8)\n"
     ]
    }
   ],
   "source": [
    "# Build x_train, y_tarin, x_test, y_test\n",
    "data_combined =  pd.concat([data_pos, data_neg])\n",
    "data_combined = data_combined.sample(frac=1)\n",
    "print('data_combined: ', data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194042, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f = data_combined.iloc[:,0:5]\n",
    "data_f.astype(float)\n",
    "labels = data_combined.iloc[:,7:8]\n",
    "labels.astype(float)\n",
    "type(labels)\n",
    "#n_values = np.max(labels) \n",
    "labels_ = np.eye(2)[(labels.values).reshape(-1)]\n",
    "labels_.shape\n",
    "# labels = labels.values\n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(labels)\n",
    "# labels= enc.transform(labels)\n",
    "# labels.shape\n",
    "#del data_combined, data_neg, data_pos\n",
    "# y = np.zeros((data_combined.shape[0], 2))\n",
    "# y[np.arange(data_combined.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_f,labels_,test_size = 0.1,random_state = 100)\n",
    "x_train.astype(float)\n",
    "x_test.astype(float)\n",
    "y_train.astype(float)\n",
    "y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_LABEL = 'y'                                   \t\t\t        # Name of the variable to be predicted\n",
    "#KEYS = [i for i in raw_data.keys().tolist() if i != Y_LABEL]\t# Name of predictors\n",
    "N_INSTANCES = data_f.shape[0]                     \t\t\t    # Number of instances\n",
    "N_INPUT = x_train.shape[1]                      \t\t\t    # Input size\n",
    "N_CLASSES = 2                                    \t\t\t    # Number of classes (output size)\n",
    "TEST_SIZE = 0.1                                    \t\t\t      # Test set size (% of dataset)\n",
    "TRAIN_SIZE = int(N_INSTANCES * (1 - TEST_SIZE))     \t\t\t    # Train size\n",
    "LEARNING_RATE = 0.001                               \t\t\t    # Learning rate\n",
    "TRAINING_EPOCHS = 40                               \t\t\t    # Number of epochs\n",
    "BATCH_SIZE = 100                                    \t\t\t    # Batch size\n",
    "DISPLAY_STEP = 5                                    \t\t\t    # Display progress each x epochs\n",
    "HIDDEN_SIZE = 10          \t                                   \t\t      # Number of hidden neurons 256\n",
    "ACTIVATION_FUNCTION_OUT = tf.nn.sigmoid                          # Last layer act fct\n",
    "STDDEV = 0.1                                        \t\t\t    # Standard deviation (for weights random init)\n",
    "RANDOM_STATE = 100   \t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = N_INPUT                   # input n labels\n",
    "n_hidden_1 = HIDDEN_SIZE            # 1st layer\n",
    "n_hidden_2 = HIDDEN_SIZE            # 2nd layer\n",
    "n_hidden_3 = HIDDEN_SIZE            # 3rd layer\n",
    "n_hidden_4 = HIDDEN_SIZE            # 4th layer\n",
    "n_classes = N_CLASSES               # output m classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input],name='X')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes],name='y')\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=STDDEV)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4],stddev=STDDEV)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes],stddev=STDDEV)),                                   \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}    \n",
    "   \n",
    "def mlp(_X, _weights, _biases, dropout_keep_prob):\n",
    "    layer1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])), dropout_keep_prob)\n",
    "    layer2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2'])), dropout_keep_prob)\n",
    "    layer3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer2, _weights['h3']), _biases['b3'])), dropout_keep_prob)\n",
    "    layer4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer3, _weights['h4']), _biases['b4'])), dropout_keep_prob)\n",
    "    out = ACTIVATION_FUNCTION_OUT(tf.add(tf.matmul(layer4, _weights['out']), _biases['out']))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlp(X, weights, biases, dropout_keep_prob)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/040 cost: 0.574416064\n",
      "Training accuracy: 0.720\n",
      "Epoch: 005/040 cost: 0.544168553\n",
      "Training accuracy: 0.730\n",
      "Epoch: 010/040 cost: 0.534956565\n",
      "Training accuracy: 0.730\n",
      "Epoch: 015/040 cost: 0.532019346\n",
      "Training accuracy: 0.710\n",
      "Epoch: 020/040 cost: 0.529941426\n",
      "Training accuracy: 0.780\n",
      "Epoch: 025/040 cost: 0.521214060\n",
      "Training accuracy: 0.770\n",
      "Epoch: 030/040 cost: 0.522534023\n",
      "Training accuracy: 0.800\n",
      "Epoch: 035/040 cost: 0.522402465\n",
      "Training accuracy: 0.780\n",
      "Test accuracy: 0.766\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(x_train.shape[0] / BATCH_SIZE)\n",
    "        X_batches = np.array_split(x_train, total_batch)\n",
    "        Y_batches = np.array_split(y_train, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "#             randidx = np.random.randint(int(TRAIN_SIZE), size = BATCH_SIZE)\n",
    "#             batch_xs = x_tr[randidx, :]\n",
    "#             batch_ys = labels[randidx, :]\n",
    "            batch_xs, batch_ys =  X_batches[i], Y_batches[i]\n",
    "            # Fit using batched data\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob: 0.9})\n",
    "            # Calculate average cost\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob:1.})/total_batch\n",
    "        # Display progress\n",
    "        if epoch % DISPLAY_STEP == 0:\n",
    "            print(\"Epoch: %03d/%03d cost: %.9f\" % (epoch, TRAINING_EPOCHS, avg_cost))\n",
    "            train_acc = sess.run(accuracy, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob:1.})\n",
    "            print(\"Training accuracy: %.3f\" % (train_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: x_test, y: y_test, dropout_keep_prob:1.})\n",
    "    print (\"Test accuracy: %.3f\" % (test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "#dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "#dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = x_train\n",
    "Y = y_train\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# larger model\n",
    "def create_larger():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(10, input_dim=x_train.shape[1], init='normal', activation='relu'))\n",
    "\tmodel.add(Dense(5, init='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, nb_epoch=100, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(y=encoded_Y, n_folds=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
